{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "\n",
    "Run first every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Written by Steve Baskauf 2020-06-29 CC0\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------\n",
    "# Configuration section\n",
    "# -----------------------\n",
    "\n",
    "# Note: to test this script, use the cv-hierarchy.csv file from the example-spreadsheets directory. Change its \n",
    "# name to pathway-revised.csv and put it in the process directory (along with this notebook).\n",
    "\n",
    "# Set these values prior to running the script\n",
    "namespaceUri = 'http://rs.tdwg.org/chrono/iri/'\n",
    "database = 'chronoiri'\n",
    "date_issued = '2021-02-02'\n",
    "local_offset_from_utc = '-05:00'\n",
    "versions = database + '-versions'\n",
    "modifications_filename = 'chronometric_revision_iri_2021-02-02.csv'\n",
    "vocab_type = 1 # 1 is simple vocabulary, 2 is simple controlled vocabulary, 3 is c.v. with broader hierarchy\n",
    "version_namespace = namespaceUri + 'version/'\n",
    "\n",
    "# For borrowed terms, specify the termlist URI here:\n",
    "\n",
    "termlist_uri = ''\n",
    "\n",
    "# For terms minted by TDWG that follow URI pattern conventions, leave this the empty string and the \n",
    "# termlist_uri will be set to the namespace URI.\n",
    "# In both cases the termlist version URI will be constructed from the namespace URI\n",
    "if termlist_uri == '':\n",
    "    termlist_uri = namespaceUri\n",
    "\n",
    "# namespace is actually the last component of the termlist URI, not of the namespace URI\n",
    "pieces = termlist_uri.split('/')\n",
    "namespace = pieces[len(pieces)-2]\n",
    "vocabulary = pieces[len(pieces)-3]\n",
    "\n",
    "# -------------\n",
    "# Define functions\n",
    "# -------------\n",
    "\n",
    "def readCsv(filename):\n",
    "    fileObject = open(filename, 'r', newline='', encoding='utf-8')\n",
    "    readerObject = csv.reader(fileObject)\n",
    "    array = []\n",
    "    for row in readerObject:\n",
    "        array.append(row)\n",
    "    fileObject.close()\n",
    "    return array\n",
    "\n",
    "def writeCsv(fileName, array):\n",
    "    fileObject = open(fileName, 'w', newline='', encoding='utf-8')\n",
    "    writerObject = csv.writer(fileObject)\n",
    "    for row in array:\n",
    "        writerObject.writerow(row)\n",
    "    fileObject.close()\n",
    "\n",
    "    # returns a list with first item Boolean and second item the index\n",
    "def findColumnWithHeader(header_row_list, header_label):\n",
    "    found = False\n",
    "    for column_number in range(0, len(header_row_list)):\n",
    "        if header_row_list[column_number] == header_label:\n",
    "            found = True\n",
    "            found_column = column_number\n",
    "    if found:\n",
    "        return [True, found_column]\n",
    "    else:\n",
    "        return [False, 0]\n",
    "    \n",
    "def isoTime(offset):\n",
    "    currentTime = datetime.datetime.now()\n",
    "    return currentTime.strftime(\"%Y-%m-%dT%H:%M:%S\") + offset\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 !!!!!! Pay attention !!! new term lists only\n",
    "\n",
    "ONLY run this with new term lists when new database directories need to be created. Otherwise, you'll write over existing data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mutable column headers from the modifications file\n",
    "modifications_metadata = readCsv(modifications_filename)\n",
    "mutable_header = modifications_metadata[0][1:len(modifications_metadata[0])]\n",
    "\n",
    "# create database directories\n",
    "try:\n",
    "    os.mkdir('../' + database)\n",
    "    os.mkdir('../' + database + '-versions')\n",
    "# do nothing if there is an error (i.e. they already exist)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# copy files needed in the current terms database directory\n",
    "endings = ['-classes.csv', '-replacements-classes.csv', '-replacements-column-mappings.csv', '-replacements.csv', '-versions-classes.csv', '-versions-column-mappings.csv', '-versions.csv']\n",
    "source_path = 'files_for_new/current_terms/'\n",
    "for file_ending in endings:\n",
    "    source = source_path + 'template' + file_ending\n",
    "    destination = '../' + database + '/' + database + file_ending\n",
    "    dest_path = shutil.copyfile(source, destination)\n",
    "dest_path = shutil.copyfile(source_path + 'namespace.csv', '../' + database + '/namespace.csv')\n",
    "\n",
    "# select current terms column mapping file appropriate for modifications spreadsheet\n",
    "if vocab_type == 1: # simple vocabulary\n",
    "    in_file = 'simple-vocabulary-column-mappings.csv'\n",
    "elif vocab_type == 2: # simple controlled vocabulary\n",
    "    in_file = 'simple-cv-column-mappings.csv'\n",
    "elif vocab_type == 3: # c.v. with skos:broader hierarchy\n",
    "    in_file = 'cv-hierarchy-column-mappings.csv'\n",
    "else: # This should not happen\n",
    "    in_file = 'simple-vocabulary-column-mappings.csv'\n",
    "\n",
    "frame = pd.read_csv(source_path + in_file, na_filter=False)\n",
    "for index,row in frame.iterrows():\n",
    "    # replace the placeholder IRIs with the namespace IRI\n",
    "    if row['header'] == 'skos_inScheme':\n",
    "        frame.at[index,'value'] = namespaceUri\n",
    "    if row['header'] == 'skos_broader':\n",
    "        frame.at[index,'value'] = namespaceUri\n",
    "frame.to_csv('../' + database + '/' + database + '-column-mappings.csv', index=False)\n",
    "    \n",
    "# set the core class file and domain root in the constants.csv configuration file\n",
    "frame = pd.read_csv(source_path + 'constants.csv', na_filter=False)\n",
    "frame.at[0,'domainRoot'] = namespaceUri\n",
    "frame.at[0,'coreClassFile'] = database + '.csv'\n",
    "frame.to_csv('../' + database + '/constants.csv', index=False)\n",
    "    \n",
    "# set the versions and replacements filenames in the linked-classes.csv file\n",
    "frame = pd.read_csv(source_path + 'linked-classes.csv', na_filter=False)\n",
    "for index,row in frame.iterrows():\n",
    "    # replace the placeholder filenames with the actual linked file names\n",
    "    if row['link_column'] == 'term_localName':\n",
    "        frame.at[index,'filename'] = database + '-versions.csv'\n",
    "    if row['link_column'] == 'replaced_term_localName':\n",
    "        frame.at[index,'filename'] = database + '-replacements.csv'\n",
    "frame.to_csv('../' + database + '/linked-classes.csv', index=False)\n",
    "    \n",
    "# create header row for current terms metadata CSV\n",
    "current_terms_header = ['document_modified', 'term_localName', 'term_isDefinedBy', 'term_created', 'term_modified', 'term_deprecated', 'replaces_term', 'replaces1_term', 'replaces2_term'] + mutable_header\n",
    "current_terms_table = [current_terms_header]\n",
    "file_path = '../' + database + '/' + database + '.csv'\n",
    "writeCsv(file_path, current_terms_table)\n",
    "\n",
    "\n",
    "# copy files needed in the versions database directory\n",
    "endings = ['-versions-classes.csv', '-versions-replacements-classes.csv', '-versions-replacements-column-mappings.csv', '-versions-replacements.csv']\n",
    "source_path = 'files_for_new/versions/'\n",
    "for file_ending in endings:\n",
    "    source = source_path + 'template' + file_ending\n",
    "    destination = '../' + database + '-versions/' + database + file_ending\n",
    "    dest_path = shutil.copyfile(source, destination)\n",
    "#dest_path = shutil.copyfile(source_path + 'linked-classes.csv', '../' + database + '-versions/linked-classes.csv')\n",
    "dest_path = shutil.copyfile(source_path + 'namespace.csv', '../' + database + '-versions/namespace.csv')\n",
    "\n",
    "# select versions column mapping file appropriate for modifications spreadsheet\n",
    "if vocab_type == 1: # simple vocabulary\n",
    "    in_file = 'simple-vocabulary-versions-column-mappings.csv'\n",
    "elif vocab_type == 2: # simple controlled vocabulary\n",
    "    in_file = 'simple-cv-versions-column-mappings.csv'\n",
    "elif vocab_type == 3: # c.v. with skos:broader hierarchy\n",
    "    in_file = 'cv-hierarchy-versions-column-mappings.csv'\n",
    "else: # This should not happen\n",
    "    in_file = 'simple-vocabulary-versions-column-mappings.csv'\n",
    "\n",
    "frame = pd.read_csv(source_path + in_file, na_filter=False)\n",
    "for index,row in frame.iterrows():\n",
    "    # replace the placeholder IRIs with the namespace IRI\n",
    "    if row['header'] == 'skos_inScheme':\n",
    "        frame.at[index,'value'] = namespaceUri\n",
    "    if row['header'] == 'skos_broader':\n",
    "        frame.at[index,'value'] = namespaceUri\n",
    "    if row['header'] == 'term_localName':\n",
    "        frame.at[index,'value'] = namespaceUri\n",
    "frame.to_csv('../' + database + '-versions/' + database + '-versions-column-mappings.csv', index=False)\n",
    "\n",
    "# set the core class file and domain root in the constants.csv configuration file\n",
    "frame = pd.read_csv(source_path + 'constants.csv', na_filter=False)\n",
    "frame.at[0,'domainRoot'] = namespaceUri + 'version/'\n",
    "frame.at[0,'coreClassFile'] = database + '-versions.csv'\n",
    "frame.to_csv('../' + database + '-versions/constants.csv', index=False)\n",
    "\n",
    "# set the versions and replacements filenames in the linked-classes.csv file\n",
    "frame = pd.read_csv(source_path + 'linked-classes.csv', na_filter=False)\n",
    "for index,row in frame.iterrows():\n",
    "    # replace the placeholder filename with the actual linked file name\n",
    "    if row['link_column'] == 'replaced_version_localName':\n",
    "        frame.at[index,'filename'] = database + '-versions-replacements.csv'\n",
    "frame.to_csv('../' + database + '-versions/linked-classes.csv', index=False)\n",
    "\n",
    "# create header row for versions metadata CSV\n",
    "versions_header = ['document_modified', 'version', 'versionLocalName', 'version_isDefinedBy', 'version_issued', 'version_status', 'replaces_version', 'replaces1_version', 'replaces2_version'] + mutable_header + ['term_localName']\n",
    "versions_table = [versions_header]\n",
    "file_path = '../' + database + '-versions/' + database + '-versions.csv'\n",
    "writeCsv(file_path, versions_table)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 \n",
    "\n",
    "Run this every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# 2.1 read tables\n",
    "terms_metadata_filename = '../' + database + '/' + database + '.csv'\n",
    "terms_metadata = readCsv(terms_metadata_filename)\n",
    "\n",
    "modifications_metadata = readCsv(modifications_filename)\n",
    "\n",
    "# find column numbers\n",
    "result = findColumnWithHeader(modifications_metadata[0], 'term_localName')\n",
    "if result[0] == False:\n",
    "    print('The modifications file does not have a term_localName column')\n",
    "    sys.exit()\n",
    "else:\n",
    "    mods_local_name = result[1]\n",
    "\n",
    "# don't error trap here because all existing files should have a local name column header\n",
    "result = findColumnWithHeader(terms_metadata[0], 'term_localName')\n",
    "metadata_localname_column = result[1]\n",
    "\n",
    "# create list of local names\n",
    "mods_term_localName = []\n",
    "for term_number in range(1, len(modifications_metadata)):\n",
    "    mods_term_localName.append(modifications_metadata[term_number][mods_local_name])\n",
    "\n",
    "# find new and modified terms\n",
    "new_terms = []\n",
    "modified_terms = []\n",
    "for test_term in mods_term_localName:\n",
    "    found = False\n",
    "    for term in terms_metadata:\n",
    "        if test_term == term[metadata_localname_column]:\n",
    "            found = True\n",
    "            modified_terms.append(test_term)\n",
    "    if not found:\n",
    "        new_terms.append(test_term)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 !!! TDWG-minted (non-borrowed) terms only !\n",
    "\n",
    "Generate versions data\n",
    "\n",
    "Only run this section for TDWG-minted terms (not borrowed from a subdomain outside rs.tdwg.org); other terms generally don't have versions. \n",
    "\n",
    "Dublin Core terms are a special case. They have versions, but those versions are managed by DCMI. So updating things like examples in our metadata cannot trigger new term version IRIs. The term version metadata must just be edited manually to reflect the changes in TDWG-specific metadata rather than using this part of the script. In particular, the `term-lists-versions-members.csv` file in the `term-lists-versions` directory will have to be manually edited to assign the same Dublin Core versions to the new term list version that was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "term_versions_metadata_filename = '../' + versions + '/' + versions + '.csv'\n",
    "term_versions_metadata = readCsv(term_versions_metadata_filename)\n",
    "\n",
    "version_modified = findColumnWithHeader(term_versions_metadata[0], 'document_modified')[1]\n",
    "version_column = findColumnWithHeader(term_versions_metadata[0], 'version')[1]\n",
    "version_local_name = findColumnWithHeader(term_versions_metadata[0], 'versionLocalName')[1]\n",
    "version_isDefinedBy = findColumnWithHeader(term_versions_metadata[0], 'version_isDefinedBy')[1]\n",
    "version_issued = findColumnWithHeader(term_versions_metadata[0], 'version_issued')[1]\n",
    "version_status = findColumnWithHeader(term_versions_metadata[0], 'version_status')[1]\n",
    "replaces_version = findColumnWithHeader(term_versions_metadata[0], 'replaces_version')[1]\n",
    "version_term_local_name_column = findColumnWithHeader(term_versions_metadata[0], 'term_localName')[1]\n",
    "\n",
    "for term in modified_terms:\n",
    "    for version_row in range(1, len(term_versions_metadata)):\n",
    "        if term_versions_metadata[version_row][version_term_local_name_column] == term and term_versions_metadata[version_row][version_status] == 'recommended':\n",
    "            term_versions_metadata[version_row][version_status] = 'superseded'\n",
    "            term_versions_metadata[version_row][version_modified] = isoTime(local_offset_from_utc)\n",
    "\n",
    "for column in modifications_metadata[0]:\n",
    "    result = findColumnWithHeader(term_versions_metadata[0], column)\n",
    "    if result[0] == False:\n",
    "        print('The versions file is missing the ', column, ' column.')\n",
    "        sys.exit()\n",
    "\n",
    "versions_join_table_filename = '../' + database + '/' + versions + '.csv'\n",
    "versions_join_table = readCsv(versions_join_table_filename)\n",
    "\n",
    "newVersions = []\n",
    "newVersionJoins = []\n",
    "\n",
    "for row_number in range(1, len(modifications_metadata)):\n",
    "    newVersion = []\n",
    "    # create a column for every column in the term version file\n",
    "    for column in term_versions_metadata[0]:\n",
    "        # find the column in the modifications file that matches the version column and add its value\n",
    "        result = findColumnWithHeader(modifications_metadata[0], column)\n",
    "        if result[0] == True:\n",
    "            newVersion.append(modifications_metadata[row_number][result[1]])\n",
    "        else:\n",
    "            newVersion.append('')\n",
    "    # set the modification dateTime for the newly created version\n",
    "    newVersion[version_modified] = isoTime(local_offset_from_utc)\n",
    "    newVersions.append(newVersion)\n",
    "\n",
    "for rowNumber in range(0, len(newVersions)):\n",
    "    # need to add one to the row of modifications_metadata because it includes a header row\n",
    "    currentTermLocalName = modifications_metadata[rowNumber + 1][mods_local_name]\n",
    "    newVersions[rowNumber][version_issued] = date_issued\n",
    "    newVersions[rowNumber][version_status] = 'recommended'\n",
    "    newVersions[rowNumber][version_local_name] = currentTermLocalName + '-' + date_issued\n",
    "    newVersions[rowNumber][version_isDefinedBy] = version_namespace\n",
    "    newVersions[rowNumber][version_column] = version_namespace + currentTermLocalName + '-' + date_issued\n",
    "\n",
    "    # if the new version replaces an older one for the term, we need to provide a value for the `replaces_version` column\n",
    "    if currentTermLocalName in modified_terms:\n",
    "        # look through metadata for old versions to find the most recent version of the term\n",
    "        mostRecent = 'a' # start with a string value earlier in alphabetization than any term version URI\n",
    "        for version_row in range(1, len(term_versions_metadata)):\n",
    "            if term_versions_metadata[version_row][version_term_local_name_column] == currentTermLocalName:\n",
    "                # Make it the mostRecent if it's later than the previous mostRecent\n",
    "                if term_versions_metadata[version_row][version_column] > mostRecent:\n",
    "                    mostRecent = term_versions_metadata[version_row][version_column]\n",
    "        # insert the most recent version found into the appropriate column\n",
    "        newVersions[rowNumber][replaces_version] = mostRecent\n",
    "    \n",
    "    # create a join record for each new version and add it to the list of new joins\n",
    "    newVersionJoin =[ newVersions[rowNumber][version_column], modifications_metadata[rowNumber + 1][mods_local_name] ]\n",
    "    newVersionJoins.append(newVersionJoin)\n",
    "\n",
    "revised_term_versions_metadata = term_versions_metadata + newVersions\n",
    "writeCsv('../' + versions + '/' + versions + '.csv', revised_term_versions_metadata)\n",
    "\n",
    "revised_term_versions_joins = versions_join_table + newVersionJoins\n",
    "writeCsv('../' + database + '/' + versions + '.csv', revised_term_versions_joins)\n",
    "\n",
    "versions_replacements_table_filename = '../' + versions + '/' + versions + '-replacements.csv'\n",
    "versions_replacements_table = readCsv(versions_replacements_table_filename)\n",
    "\n",
    "# create a list to hold the newly generated replacements rows\n",
    "newReplacements = []\n",
    "\n",
    "for modifiedTerm in modified_terms:\n",
    "    # generate the newly created version URI for the modified term\n",
    "    newVersion = version_namespace + modifiedTerm  + '-' + date_issued\n",
    "    # step through the list of previous versions and find the one with the most recent issued date\n",
    "    mostRecent = 'a'\n",
    "    count = 0\n",
    "    for oldVersion in versions_join_table:\n",
    "        if count > 0: # skip the header row\n",
    "            # the second column in the join table is the term local name\n",
    "            if oldVersion[1] == modifiedTerm:\n",
    "                # the first column in the join table is the full version URI\n",
    "                if oldVersion[0] > mostRecent:\n",
    "                    mostRecent = oldVersion[0]\n",
    "        count +=1\n",
    "    # once the most revent version URI is found, we need to extract the local name\n",
    "    mostRecentLocal = mostRecent.split('/')[6]\n",
    "    newReplacements.append([newVersion, mostRecentLocal])\n",
    "\n",
    "revised_versions_replacements_table = versions_replacements_table + newReplacements\n",
    "writeCsv('../' + versions + '/' + versions + '-replacements.csv', revised_versions_replacements_table)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 \n",
    "\n",
    "Run this every time to generate current terms metadata\n",
    "\n",
    "NOTE: If this is a new term list, the correct term list metadata for the new list must be present in the file `files_for_new/new_term_list.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2021-05-04T21:23:10-05:00', 'http://rs.tdwg.org/chrono/iri/', '', 'chrono/iri/', 'Darwin Core chronometric age extension (IRI-values)', 'Chronometric age extension (IRI values) for Darwin Core', '2021-01-26', '2021-02-02', 'chronoiri', 'http://rs.tdwg.org/chrono/iri/', 'chronoiri', 'chronoiri-versions', 'http://rs.tdwg.org/chrono/version/iri/', 'http://www.tdwg.org/standards/450']\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "term_modified_dateTime = findColumnWithHeader(terms_metadata[0], 'document_modified')[1]\n",
    "term_localName = findColumnWithHeader(terms_metadata[0], 'term_localName')[1]\n",
    "term_modified = findColumnWithHeader(terms_metadata[0], 'term_modified')[1]\n",
    "term_created = findColumnWithHeader(terms_metadata[0], 'term_created')[1]\n",
    "term_isDefinedBy = findColumnWithHeader(terms_metadata[0], 'term_isDefinedBy')[1]\n",
    "\n",
    "# step through each row in the modification metadata table and modify existing current terms when applicable\n",
    "for mods_rownumber in range(1, len(modifications_metadata)):\n",
    "    mods_localname_string = modifications_metadata[mods_rownumber][mods_local_name]\n",
    "    modified = False\n",
    "    for term_name in modified_terms:\n",
    "        # only make a modification if it's on the list of terms to be modified\n",
    "        if mods_localname_string == term_name:\n",
    "            modified = True\n",
    "    # this section of code modifies existing terms\n",
    "    if modified:\n",
    "        # find the row in the terms metadata file for the term to be modified\n",
    "        for term_rownumber in range(1, len(terms_metadata)):\n",
    "            if mods_localname_string == terms_metadata[term_rownumber][term_localName]:\n",
    "                terms_metadata[term_rownumber][term_modified_dateTime] = isoTime(local_offset_from_utc)\n",
    "                terms_metadata[term_rownumber][term_modified] = date_issued\n",
    "                # replace every column that's in the modifications metadata\n",
    "                for column_number in range(0, len(modifications_metadata[0])):\n",
    "                    # find the column in the current terms metadata table that matches the modifications column and replace the current term's value\n",
    "                    result = findColumnWithHeader(terms_metadata[0], modifications_metadata[0][column_number])\n",
    "                    if result[0] == True:\n",
    "                        terms_metadata[term_rownumber][result[1]] = modifications_metadata[mods_rownumber][column_number]\n",
    "                    else:\n",
    "                        pass # this shouldn't really happen since there already was a check that all columns existed in the versions table\n",
    "    # this section of code adds new term metadata\n",
    "    else: \n",
    "        newTermRow = []\n",
    "        for column in range(0, len(terms_metadata[0])):\n",
    "            newTermRow.append('')\n",
    "        newTermRow[term_modified_dateTime] = isoTime(local_offset_from_utc)\n",
    "        newTermRow[term_modified] = date_issued\n",
    "        newTermRow[term_created] = date_issued\n",
    "        newTermRow[term_isDefinedBy] = namespaceUri\n",
    "        # replace every column that's in the modifications metadata\n",
    "        for column_number in range(0, len(modifications_metadata[0])):\n",
    "            # find the column in the current terms metadata table that matches the modifications column and replace the current term's value\n",
    "            result = findColumnWithHeader(terms_metadata[0], modifications_metadata[0][column_number])\n",
    "            if result[0] == True:\n",
    "                newTermRow[result[1]] = modifications_metadata[mods_rownumber][column_number]\n",
    "            else:\n",
    "                pass # this shouldn't really happen since there already was a check that all columns existed in the versions table\n",
    "        terms_metadata.append(newTermRow)\n",
    "writeCsv('../' + database + '/' + database + '.csv', terms_metadata)\n",
    "\n",
    "# Section 5 for generating new term lists\n",
    "term_lists_table_filename = '../term-lists/term-lists.csv'\n",
    "term_lists_table = readCsv(term_lists_table_filename)\n",
    "\n",
    "term_lists_versions_joins_filename = '../term-lists/term-lists-versions.csv'\n",
    "term_lists_versions_joins = readCsv(term_lists_versions_joins_filename)\n",
    "\n",
    "term_lists_members_filename = '../term-lists/term-lists-members.csv'\n",
    "term_lists_members = readCsv(term_lists_members_filename)\n",
    "\n",
    "term_lists_versions_metadata_filename = '../term-lists-versions/term-lists-versions.csv'\n",
    "term_lists_versions_metadata = readCsv(term_lists_versions_metadata_filename)\n",
    "\n",
    "term_lists_versions_members_filename = '../term-lists-versions/term-lists-versions-members.csv'\n",
    "term_lists_versions_members = readCsv(term_lists_versions_members_filename)\n",
    "\n",
    "term_lists_versions_replacements_filename = '../term-lists-versions/term-lists-versions-replacements.csv'\n",
    "term_lists_versions_replacements = readCsv(term_lists_versions_replacements_filename)\n",
    "\n",
    "datasets_index_filename = '../index/index-datasets.csv'\n",
    "datasets_index = readCsv(datasets_index_filename)\n",
    "\n",
    "if namespaceUri == 'http://rs.tdwg.org/dwc/terms/attributes/':\n",
    "    termlistVersionUri = 'http://rs.tdwg.org/dwc/version/terms/attributes/' + date_issued\n",
    "else:\n",
    "    uriPieces = termlist_uri.split('/')\n",
    "    # split the URI between the vocabulary and term list subpaths\n",
    "    termlistVersionUri = uriPieces[0] + '//' + uriPieces[2] + '/' + uriPieces[3] + '/version/' + uriPieces[4] + '/' + date_issued\n",
    "\n",
    "list_uri = findColumnWithHeader(term_lists_table[0], 'list')[1]\n",
    "list_created = findColumnWithHeader(term_lists_table[0], 'list_created')[1]\n",
    "list_modified = findColumnWithHeader(term_lists_table[0], 'list_modified')[1]\n",
    "modified_datetime = findColumnWithHeader(term_lists_table[0], 'document_modified')[1]\n",
    "standard_column = findColumnWithHeader(term_lists_table[0], 'standard')[1]\n",
    "list_description = findColumnWithHeader(term_lists_table[0], 'description')[1]\n",
    "\n",
    "aNewTermList = True\n",
    "for rowNumber in range(1, len(term_lists_table)):\n",
    "    # by convention, the namespace URI used for the terms is the same as the URI of the term list\n",
    "    if termlist_uri == term_lists_table[rowNumber][list_uri]:\n",
    "        aNewTermList = False\n",
    "        term_list_rowNumber = rowNumber\n",
    "        term_lists_table[rowNumber][list_modified] = date_issued\n",
    "        term_lists_table[rowNumber][modified_datetime] = isoTime(local_offset_from_utc)\n",
    "        # here is the opportunity to find out the standard URI for the modified term list\n",
    "        standardUri = term_lists_table[rowNumber][standard_column]\n",
    "        print(term_lists_table[rowNumber])\n",
    "if aNewTermList:  # this will happen if the term list did not previously exist\n",
    "    try:\n",
    "        new_term_list = readCsv('files_for_new/new_term_list.csv')\n",
    "    except:\n",
    "        print('The term list was not found and there was no new_term_list.csv file.')\n",
    "        sys.exit()\n",
    "    # Note: no error trapping is done here, so make sure that the new_term_list columns are the same as term_lists_table\n",
    "    new_term_list[1][modified_datetime] = isoTime(local_offset_from_utc)\n",
    "    new_term_list[1][list_created] = date_issued\n",
    "    new_term_list[1][list_modified] = date_issued\n",
    "    standardUri = new_term_list[1][standard_column]\n",
    "    # the length of the table (including header row) will be one more than the last row number\n",
    "    term_list_rowNumber = len(term_lists_table)\n",
    "    term_lists_table.append(new_term_list[1])\n",
    "    # after the new row is appended, its row number will be one more than the previous last row number\n",
    "\n",
    "    # The new term list's dataset directory must be added to the dataset list. \n",
    "    row_for_current_terms = [isoTime(local_offset_from_utc), # document_modified\n",
    "                             database, # term_localName\n",
    "                             'http://rs.tdwg.org/index', # dcterms_isPartOf\n",
    "                             'http://rs.tdwg.org/index/' + database, # dataset_iri\n",
    "                             date_issued, # dcterms_modified\n",
    "                             new_term_list[1][list_description], # label\n",
    "                             ''] # rdfs_comment\n",
    "    datasets_index.append(row_for_current_terms)\n",
    "    \n",
    "    # New term lists will always have a new version dataset directory, so add it, too.\n",
    "    row_for_versions = [isoTime(local_offset_from_utc), # document_modified\n",
    "                        versions, # term_localName\n",
    "                        'http://rs.tdwg.org/index', # dcterms_isPartOf\n",
    "                        'http://rs.tdwg.org/index/' + versions, # dataset_iri\n",
    "                        date_issued, # dcterms_modified\n",
    "                        new_term_list[1][list_description] + ' versions', # label\n",
    "                        ''] # rdfs_comment\n",
    "    datasets_index.append(row_for_versions)\n",
    "    \n",
    "else: # If the term list isn't new, then its modified date needs to be updated.\n",
    "    # find the row in the dataset director file for the dataset being modified\n",
    "    for dataset_rownumber in range(1, len(datasets_index)):\n",
    "        # update current terms modified date\n",
    "        if database == datasets_index[dataset_rownumber][1]: # the name is in column 1\n",
    "            datasets_index[dataset_rownumber][0] = isoTime(local_offset_from_utc)\n",
    "            datasets_index[dataset_rownumber][4] = date_issued # the date modified is in column 4\n",
    "        # update versions modified date\n",
    "        if versions == datasets_index[dataset_rownumber][1]:\n",
    "            datasets_index[dataset_rownumber][0] = isoTime(local_offset_from_utc)\n",
    "            datasets_index[dataset_rownumber][4] = date_issued\n",
    "\n",
    "# Update the date for the term lists and term list versions regardless of whether it's new or not\n",
    "for dataset_rownumber in range(1, len(datasets_index)):\n",
    "    if 'term-lists' == datasets_index[dataset_rownumber][1]:\n",
    "        datasets_index[dataset_rownumber][0] = isoTime(local_offset_from_utc)\n",
    "        datasets_index[dataset_rownumber][4] = date_issued\n",
    "    if 'term-lists-versions' == datasets_index[dataset_rownumber][1]:\n",
    "        datasets_index[dataset_rownumber][0] = isoTime(local_offset_from_utc)\n",
    "        datasets_index[dataset_rownumber][4] = date_issued\n",
    "    if 'vocabularies' == datasets_index[dataset_rownumber][1]:\n",
    "        datasets_index[dataset_rownumber][0] = isoTime(local_offset_from_utc)\n",
    "        datasets_index[dataset_rownumber][4] = date_issued\n",
    "    if 'vocabularies-versions' == datasets_index[dataset_rownumber][1]:\n",
    "        datasets_index[dataset_rownumber][0] = isoTime(local_offset_from_utc)\n",
    "        datasets_index[dataset_rownumber][4] = date_issued\n",
    "    if 'standards' == datasets_index[dataset_rownumber][1]:\n",
    "        datasets_index[dataset_rownumber][0] = isoTime(local_offset_from_utc)\n",
    "        datasets_index[dataset_rownumber][4] = date_issued\n",
    "    if 'standards-versions' == datasets_index[dataset_rownumber][1]:\n",
    "        datasets_index[dataset_rownumber][0] = isoTime(local_offset_from_utc)\n",
    "        datasets_index[dataset_rownumber][4] = date_issued\n",
    "           \n",
    "writeCsv('../term-lists/term-lists.csv', term_lists_table)\n",
    "writeCsv('../index/index-datasets.csv', datasets_index)\n",
    "\n",
    "term_lists_versions_joins.append([termlistVersionUri, termlist_uri])\n",
    "writeCsv('../term-lists/term-lists-versions.csv', term_lists_versions_joins)\n",
    "\n",
    "for newTerm in new_terms:\n",
    "    term_lists_members.append([termlist_uri, namespaceUri + newTerm])\n",
    "writeCsv('../term-lists/term-lists-members.csv', term_lists_members)\n",
    "\n",
    "# find the columns than contain needed information\n",
    "list_uri = findColumnWithHeader(term_lists_versions_metadata[0], 'list')[1]\n",
    "document_modified = findColumnWithHeader(term_lists_versions_metadata[0], 'document_modified')[1]\n",
    "version_uri = findColumnWithHeader(term_lists_versions_metadata[0], 'version')[1]\n",
    "version_modified = findColumnWithHeader(term_lists_versions_metadata[0], 'version_modified')[1]\n",
    "status_column = findColumnWithHeader(term_lists_versions_metadata[0], 'status')[1]\n",
    "\n",
    "if aNewTermList:\n",
    "    # get the template for the term list version from first data row in the new_term_list_version.csv file\n",
    "    try:\n",
    "        new_term_list_version = readCsv('files_for_new/new_term_list_version.csv')\n",
    "    except:\n",
    "        print('The term list version was not found and there was no new_term_list_version.csv file.')\n",
    "        sys.exit()\n",
    "    newListRow = new_term_list_version[1]\n",
    "else:\n",
    "    # find the most recent previous version of the term list\n",
    "    mostRecent = 'a' # start the value of mostRecent as something earlier alphabetically than all of the list URIs\n",
    "    mostRecentListNumber = 0 # dummy list number to be replaced when most recent list is found\n",
    "    for termListRowNumber in range(1, len(term_lists_versions_metadata)):\n",
    "        # the row is one of the versions of the list\n",
    "        if term_lists_versions_metadata[termListRowNumber][list_uri] == termlist_uri:\n",
    "            # Make the version of the row the mostRecent if it's later than the previous mostRecent\n",
    "            if term_lists_versions_metadata[termListRowNumber][version_uri] > mostRecent:\n",
    "                mostRecent = term_lists_versions_metadata[termListRowNumber][version_uri]\n",
    "                mostRecentListNumber = termListRowNumber\n",
    "\n",
    "    # change the status of the most recent list to superseded\n",
    "    term_lists_versions_metadata[mostRecentListNumber][status_column] = 'superseded'\n",
    "    term_lists_versions_metadata[mostRecentListNumber][document_modified] = isoTime(local_offset_from_utc)\n",
    "\n",
    "    # start the new list row with the metadata from the most recent list\n",
    "    newListRow = copy.deepcopy(term_lists_versions_metadata[mostRecentListNumber])\n",
    "\n",
    "# substitute metadata to make the most recent list have the modified dates for the new list\n",
    "newListRow[document_modified] = isoTime(local_offset_from_utc)\n",
    "newListRow[version_uri] = termlistVersionUri\n",
    "newListRow[version_modified] = date_issued\n",
    "newListRow[status_column] = 'recommended'\n",
    "\n",
    "# append the new term list row to the old list of term lists\n",
    "term_lists_versions_metadata.append(newListRow)\n",
    "\n",
    "# save as a file\n",
    "writeCsv('../term-lists-versions/term-lists-versions.csv', term_lists_versions_metadata)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 !!! TDWG-minted (non-borrowed) terms only !\n",
    "\n",
    "Do not run this code block for borrowed terms that do not have versions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# create a list of every term version that was in the most recent previous list version\n",
    "newTermVersionMembersList = []\n",
    "# create a corresponding list of local names for those versions\n",
    "termLocalNameList = []\n",
    "\n",
    "if not aNewTermList:\n",
    "    for termVersion in term_lists_versions_members:\n",
    "        # the first column contains the term list version\n",
    "        if term_lists_versions_metadata[mostRecentListNumber][version_uri] == termVersion[0]:\n",
    "            newTermVersionMembersList.append(termVersion[1])\n",
    "\n",
    "            # dissect the term version URI to pull out the local name of the term version\n",
    "            pieces = termVersion[1].split('/')\n",
    "            versionLocalNamePiece = pieces[len(pieces)-1]\n",
    "            # split off the local name string from the issue date part of the version local name\n",
    "            termLocalNameList.append(versionLocalNamePiece.split('-')[0])\n",
    "\n",
    "    # For each modified term, find its previous version and replace it with the new version.\n",
    "    for modified_term in modified_terms:\n",
    "        for termVersionRowNumber in range(0, len(newTermVersionMembersList)):\n",
    "            if modified_term == termLocalNameList[termVersionRowNumber]:\n",
    "                # change the version on the list to the new one\n",
    "                newTermVersionMembersList[termVersionRowNumber] = namespaceUri + 'version/' + termLocalNameList[termVersionRowNumber] + '-' + date_issued\n",
    "\n",
    "# For each newly added term, add its new version to the list.\n",
    "for new_term in new_terms:\n",
    "    newTermVersionMembersList.append(namespaceUri + 'version/' + new_term + '-' + date_issued)\n",
    "\n",
    "# Now that the list is created of new term versions that are part of the new term version list,\n",
    "# add a record for each one to the term list versions members table\n",
    "for termVersionMember in newTermVersionMembersList:\n",
    "    term_lists_versions_members.append([termlistVersionUri, termVersionMember])\n",
    "\n",
    "# Write the updated term list versions members table to a file\n",
    "writeCsv('../term-lists-versions/term-lists-versions-members.csv', term_lists_versions_members)\n",
    "\n",
    "if not aNewTermList:\n",
    "    term_lists_versions_replacements.append([termlistVersionUri, term_lists_versions_metadata[mostRecentListNumber][version_uri]])\n",
    "    writeCsv('../term-lists-versions/term-lists-versions-replacements.csv', term_lists_versions_replacements)\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7\n",
    "\n",
    "It is not necessary to run this step or any following steps when just generating term list Markdown pages or testing. It needs to be run when ready to generate new versions of the vocabulary and standard.\n",
    "\n",
    "This section should NOT be run for \"utility\" type namespaces that are not part of any vocabulary or standard. Examples are `tdwgutility:` (`http://rs.tdwg.org/dwc/terms/attributes/`) and the decisions namepace (`http://rs.tdwg.org/decisions/`).\n",
    "\n",
    "NOTE: when new vocabularies or standards are being created, the appropriate files in the `files_for_new` subdirectory must be modified!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabularies_table_filename = '../vocabularies/vocabularies.csv'\n",
    "vocabularies_table = readCsv(vocabularies_table_filename)\n",
    "\n",
    "vocabularies_versions_joins_filename = '../vocabularies/vocabularies-versions.csv'\n",
    "vocabularies_versions_joins = readCsv(vocabularies_versions_joins_filename)\n",
    "\n",
    "vocabularies_members_filename = '../vocabularies/vocabularies-members.csv'\n",
    "vocabularies_members = readCsv(vocabularies_members_filename)\n",
    "\n",
    "vocabularies_versions_metadata_filename = '../vocabularies-versions/vocabularies-versions.csv'\n",
    "vocabularies_versions_metadata = readCsv(vocabularies_versions_metadata_filename)\n",
    "\n",
    "vocabularies_versions_members_filename = '../vocabularies-versions/vocabularies-versions-members.csv'\n",
    "vocabularies_versions_members = readCsv(vocabularies_versions_members_filename)\n",
    "\n",
    "vocabularies_versions_replacements_filename = '../vocabularies-versions/vocabularies-versions-replacements.csv'\n",
    "vocabularies_versions_replacements = readCsv(vocabularies_versions_replacements_filename)\n",
    "\n",
    "# find the vocabulary subpath for the updated term list\n",
    "list_localName_column = findColumnWithHeader(term_lists_table[0], 'list_localName')[1]\n",
    "list_localName = term_lists_table[term_list_rowNumber][list_localName_column]\n",
    "# the vocabulary subpath is the first part of the list local name\n",
    "vocab_subpath = list_localName.split('/')[0]\n",
    "termList_subpath = list_localName.split('/')[1]\n",
    "\n",
    "# generate the vocabulary URI\n",
    "vocabularyUri = 'http://rs.tdwg.org/' + vocab_subpath + '/'\n",
    "\n",
    "# generate the vocabulary version URI\n",
    "vocabularyVersionUri = 'http://rs.tdwg.org/version/' + vocab_subpath + '/' + date_issued\n",
    "\n",
    "# check for the case where the script was previously run to update a different term list in the same new vocabulary version\n",
    "temp = findColumnWithHeader(vocabularies_versions_metadata[0], 'version')[1]\n",
    "alreadyAddedVocab = False\n",
    "for versionRow in vocabularies_versions_metadata:\n",
    "    if versionRow[temp] == vocabularyVersionUri:\n",
    "        alreadyAddedVocab = True\n",
    "\n",
    "vocabulary_uri = findColumnWithHeader(vocabularies_table[0], 'vocabulary')[1]\n",
    "vocabulary_created = findColumnWithHeader(vocabularies_table[0], 'vocabulary_created')[1]\n",
    "vocabulary_modified = findColumnWithHeader(vocabularies_table[0], 'vocabulary_modified')[1]\n",
    "modified_datetime = findColumnWithHeader(vocabularies_table[0], 'document_modified')[1]\n",
    "\n",
    "aNewVocabulary = True\n",
    "for rowNumber in range(1, len(vocabularies_table)):\n",
    "    if vocabularyUri == vocabularies_table[rowNumber][vocabulary_uri]:\n",
    "        aNewVocabulary = False\n",
    "        vocabulary_rowNumber = rowNumber\n",
    "        # In the case where changes are made to a second term list of a new vocabulary, the new modified date will be the same as before\n",
    "        vocabularies_table[rowNumber][vocabulary_modified] = date_issued\n",
    "        vocabularies_table[rowNumber][modified_datetime] = isoTime(local_offset_from_utc)\n",
    "\n",
    "if aNewVocabulary: # this will happen if the vocabulary did not previously exist \n",
    "    try:\n",
    "        new_vocabulary_row = readCsv('files_for_new/new_vocabulary.csv')[1]\n",
    "    except:\n",
    "        print('The vocabulary was not found and there was no new_vocabulary.csv file.')\n",
    "        sys.exit()\n",
    "    new_vocabulary_row[vocabulary_created] = date_issued\n",
    "    new_vocabulary_row[vocabulary_modified] = date_issued\n",
    "    new_vocabulary_row[modified_datetime] = isoTime(local_offset_from_utc)\n",
    "    vocabularies_table.append(new_vocabulary_row)\n",
    "\n",
    "writeCsv('../vocabularies/vocabularies.csv', vocabularies_table)\n",
    "\n",
    "if not alreadyAddedVocab:\n",
    "    vocabularies_versions_joins.append([vocabularyVersionUri, vocabularyUri])\n",
    "    writeCsv('../vocabularies/vocabularies-versions.csv', vocabularies_versions_joins)\n",
    "\n",
    "if aNewTermList:\n",
    "    vocabularies_members.append([vocabularyUri, termlist_uri])\n",
    "    writeCsv('../vocabularies/vocabularies-members.csv', vocabularies_members)\n",
    "\n",
    "# find the columns than contain needed information\n",
    "vocabulary_uri = findColumnWithHeader(vocabularies_versions_metadata[0], 'vocabulary')[1]\n",
    "document_modified = findColumnWithHeader(vocabularies_versions_metadata[0], 'document_modified')[1]\n",
    "version_uri = findColumnWithHeader(vocabularies_versions_metadata[0], 'version')[1]\n",
    "version_issued = findColumnWithHeader(vocabularies_versions_metadata[0], 'version_issued')[1]\n",
    "status_column = findColumnWithHeader(vocabularies_versions_metadata[0], 'vocabulary_status')[1]\n",
    "\n",
    "if not alreadyAddedVocab:\n",
    "    if aNewVocabulary: # this will happen if the vocabulary did not previously exist \n",
    "        try:\n",
    "            newVocabularyRow = readCsv('files_for_new/new_vocabulary_version.csv')[1]\n",
    "        except:\n",
    "            print('The vocabulary version was not found and there was no new_vocabulary_version.csv file.')\n",
    "            sys.exit()\n",
    "        # the new row will be added to the end and therefore will have an index number - number of rows before appending\n",
    "        mostRecentVocabularyNumber = len(vocabularies_versions_metadata)\n",
    "    else:\n",
    "        # find the most recent previous version of the vocabulary\n",
    "        mostRecent = 'a' # start the value of mostRecent as something earlier alphabetically than all of the vocabulary version URIs\n",
    "        mostRecentVocabularyNumber = 0 # dummy vocabulary number to be replaced when most recent vocabulary version is found\n",
    "        for vocabularyRowNumber in range(1, len(vocabularies_versions_metadata)):\n",
    "            # the row is one of the versions of the vocabulary\n",
    "            if vocabularies_versions_metadata[vocabularyRowNumber][vocabulary_uri] == vocabularyUri:\n",
    "                # Make the version of the row the mostRecent if it's later than the previous mostRecent\n",
    "                if vocabularies_versions_metadata[vocabularyRowNumber][version_uri] > mostRecent:\n",
    "                    mostRecent = vocabularies_versions_metadata[vocabularyRowNumber][version_uri]\n",
    "                    mostRecentVocabularyNumber = vocabularyRowNumber\n",
    "\n",
    "        # change the status of the most recent vocabulary to superseded\n",
    "        vocabularies_versions_metadata[mostRecentVocabularyNumber][status_column] = 'superseded'\n",
    "        vocabularies_versions_metadata[mostRecentVocabularyNumber][document_modified] = isoTime(local_offset_from_utc)\n",
    "\n",
    "        # start the new vocabulary row with the metadata from the most recent vocabulary\n",
    "        newVocabularyRow = copy.deepcopy(vocabularies_versions_metadata[mostRecentVocabularyNumber])\n",
    "\n",
    "    # substitute metadata to make the most recent vocabulary have the modified dates for the new vocabulary\n",
    "    newVocabularyRow[document_modified] = isoTime(local_offset_from_utc)\n",
    "    newVocabularyRow[version_uri] = vocabularyVersionUri\n",
    "    newVocabularyRow[version_issued] = date_issued\n",
    "    newVocabularyRow[status_column] = 'recommended'\n",
    "\n",
    "    # append the new term list row to the old list of term lists\n",
    "    vocabularies_versions_metadata.append(newVocabularyRow)\n",
    "\n",
    "    # save as a file\n",
    "    writeCsv('../vocabularies-versions/vocabularies-versions.csv', vocabularies_versions_metadata)\n",
    "\n",
    "# If this is the second term list change for a new vocabulary version, the previous term list versions will already have been added.\n",
    "# So they don't need to be added to the list.  \n",
    "if not alreadyAddedVocab:\n",
    "    # create a list of every term list version that was in the most recent previous vocabulary version\n",
    "    newVocabularyMembersList = []\n",
    "    # create a corresponding list of local names for those term list versions\n",
    "    termListLocalNameList = []\n",
    "\n",
    "    if aNewVocabulary:\n",
    "        # the new term list version should be added to the list\n",
    "        newVocabularyMembersList.append(termlistVersionUri)\n",
    "    else:\n",
    "        # find all of the term list versions for the most recent vocabulary version\n",
    "        for termListVersion in vocabularies_versions_members:\n",
    "            # the first column contains the vocabulary version\n",
    "            if vocabularies_versions_metadata[mostRecentVocabularyNumber][version_uri] == termListVersion[0]:\n",
    "                newVocabularyMembersList.append(termListVersion[1])\n",
    "\n",
    "                # dissect the term list version URI to pull out the local name of the term list version\n",
    "                pieces = termListVersion[1].split('/')\n",
    "                versionLocalNamePiece = pieces[len(pieces)-2]\n",
    "                termListLocalNameList.append(versionLocalNamePiece)\n",
    "        if aNewTermList:\n",
    "            # the new term list version needs be added to the list\n",
    "            newVocabularyMembersList.append(termlistVersionUri)\n",
    "        else:\n",
    "            # For the modified term list, find its previous version and replace it with the new new version.\n",
    "            for termListVersionRowNumber in range(0, len(newVocabularyMembersList)):\n",
    "                if termList_subpath == termListLocalNameList[termListVersionRowNumber]:\n",
    "                    # change the term list version on the list to the new one\n",
    "                    newVocabularyMembersList[termListVersionRowNumber] = termlistVersionUri\n",
    "    \n",
    "    # Now that the list of new term list versions that are part of the new vocabulary version list is created,\n",
    "    # add a record for each one to the vocabulary versions members table\n",
    "    for termListVersionMember in newVocabularyMembersList:\n",
    "        vocabularies_versions_members.append([vocabularyVersionUri, termListVersionMember])\n",
    "\n",
    "# In the case where previous term list versions have already been added and a new vocabulary version already generated, \n",
    "# we only need to update the new term list version.\n",
    "else: \n",
    "    if aNewTermList:\n",
    "        # the new term list version needs be added to the list\n",
    "        vocabularies_versions_members.append([vocabularyVersionUri, termlistVersionUri])\n",
    "    else:\n",
    "        # For a modified term list, find its previous version and replace it with the new version.\n",
    "        for termListVersionRowNumber in range(1, len(vocabularies_versions_members)):\n",
    "            # consider only term lists that match the vocabulary version URI\n",
    "            if vocabularies_versions_members[termListVersionRowNumber][0] == vocabularyVersionUri:\n",
    "                # dissect the term list version URI to pull out the local name of the term list version\n",
    "                pieces = vocabularies_versions_members[termListVersionRowNumber][1].split('/')\n",
    "                versionLocalNamePiece = pieces[len(pieces)-2]\n",
    "                # check for a match of the term list version local name with the namespace string\n",
    "                if versionLocalNamePiece == namespace:\n",
    "                    # change the term list version on the list to the new one\n",
    "                    vocabularies_versions_members[termListVersionRowNumber][1] = termlistVersionUri\n",
    "    \n",
    "# Write the updated vocabularies versions members table to a file\n",
    "writeCsv('../vocabularies-versions/vocabularies-versions-members.csv', vocabularies_versions_members)\n",
    "\n",
    "if not(aNewVocabulary) and not(alreadyAddedVocab):\n",
    "    vocabularies_versions_replacements.append([vocabularyVersionUri, vocabularies_versions_metadata[mostRecentVocabularyNumber][version_uri]])\n",
    "    writeCsv('../vocabularies-versions/vocabularies-versions-replacements.csv', vocabularies_versions_replacements)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beginning of section 7 to generate the new standard version\n",
    "standards_table_filename = '../standards/standards.csv'\n",
    "standards_table = readCsv(standards_table_filename)\n",
    "\n",
    "standards_versions_joins_filename = '../standards/standards-versions.csv'\n",
    "standards_versions_joins = readCsv(standards_versions_joins_filename)\n",
    "\n",
    "standards_parts_filename = '../standards/standards-parts.csv'\n",
    "standards_parts = readCsv(standards_parts_filename)\n",
    "\n",
    "standards_versions_metadata_filename = '../standards-versions/standards-versions.csv'\n",
    "standards_versions_metadata = readCsv(standards_versions_metadata_filename)\n",
    "\n",
    "standards_versions_parts_filename = '../standards-versions/standards-versions-parts.csv'\n",
    "standards_versions_parts = readCsv(standards_versions_parts_filename)\n",
    "\n",
    "standards_versions_replacements_filename = '../standards-versions/standards-versions-replacements.csv'\n",
    "standards_versions_replacements = readCsv(standards_versions_replacements_filename)\n",
    "\n",
    "# the standard URI (variable: standardUri) was already found in section 5.2 above\n",
    "\n",
    "# find the standard number for the standard\n",
    "standard_number = standardUri.split('/')[4]\n",
    "\n",
    "# generate the standard version URI\n",
    "standardVersionUri = standardUri + '/version/' + date_issued\n",
    "\n",
    "# check for the case where the script was previously run to update a different term list in the same new standard version\n",
    "temp = findColumnWithHeader(standards_versions_metadata[0], 'version')[1]\n",
    "alreadyAddedStandard = False\n",
    "for versionRow in standards_versions_metadata:\n",
    "    if versionRow[temp] == standardVersionUri:\n",
    "        alreadyAddedStandard = True\n",
    "\n",
    "standard_uri = findColumnWithHeader(standards_table[0], 'standard')[1]\n",
    "standard_created = findColumnWithHeader(standards_table[0], 'standard_created')[1]\n",
    "standard_modified = findColumnWithHeader(standards_table[0], 'standard_modified')[1]\n",
    "modified_datetime = findColumnWithHeader(standards_table[0], 'document_modified')[1]\n",
    "\n",
    "aNewStandard = True\n",
    "for rowNumber in range(1, len(standards_table)):\n",
    "    if standardUri == standards_table[rowNumber][standard_uri]:\n",
    "        aNewStandard = False\n",
    "        standard_rowNumber = rowNumber\n",
    "        # in cases where changes are made to a second term list of a new standard, the new modified date will be the same as before\n",
    "        standards_table[rowNumber][standard_modified] = date_issued\n",
    "        standards_table[rowNumber][modified_datetime] = isoTime(local_offset_from_utc)\n",
    "\n",
    "if aNewStandard: # this will happen if the standard did not previously exist \n",
    "    try:\n",
    "        new_standard_row = readCsv('files_for_new/new_standard.csv')[1]\n",
    "    except:\n",
    "        print('The standard was not found and there was no new_standard.csv file.')\n",
    "        sys.exit()\n",
    "    new_standard_row[standard_created] = date_issued\n",
    "    new_standard_row[standard_modified] = date_issued\n",
    "    new_standard_row[modified_datetime] = isoTime(local_offset_from_utc)\n",
    "    # the row is set to what the last row will be after appending\n",
    "    standard_rowNumber = len(standards_table)\n",
    "    standards_table.append(new_standard_row)\n",
    "\n",
    "writeCsv('../standards/standards.csv', standards_table)\n",
    "\n",
    "if not alreadyAddedStandard:\n",
    "    standards_versions_joins.append([standardVersionUri, standardUri])\n",
    "    writeCsv('../standards/standards-versions.csv', standards_versions_joins)\n",
    "\n",
    "if aNewVocabulary:\n",
    "    standards_parts.append([standardUri, vocabularyUri, 'tdwgutility:Vocabulary'])\n",
    "    writeCsv('../standards/standards-parts.csv', standards_parts)\n",
    "\n",
    "# find the columns than contain needed information\n",
    "standard_uri = findColumnWithHeader(standards_versions_metadata[0], 'standard')[1]\n",
    "document_modified = findColumnWithHeader(standards_versions_metadata[0], 'document_modified')[1]\n",
    "version_uri = findColumnWithHeader(standards_versions_metadata[0], 'version')[1]\n",
    "version_issued = findColumnWithHeader(standards_versions_metadata[0], 'version_issued')[1]\n",
    "status_column = findColumnWithHeader(standards_versions_metadata[0], 'standard_status')[1]\n",
    "\n",
    "if not alreadyAddedStandard:\n",
    "    if aNewStandard: # this will happen if the standard did not previously exist \n",
    "        try:\n",
    "            newStandardRow = readCsv('files_for_new/new_standard_version.csv')[1]\n",
    "        except:\n",
    "            print('The standard version was not found and there was no new_standard_version.csv file.')\n",
    "            sys.exit()\n",
    "        # the new row will be added to the end and therefore will have an index number - number of rows before appending\n",
    "        mostRecentStandardNumber = len(standards_versions_metadata)\n",
    "    else:\n",
    "        # find the most recent previous version of the standard\n",
    "        mostRecent = 'a' # start the value of mostRecent as something earlier alphabetically than all of the standard version URIs\n",
    "        mostRecentStandardNumber = 0 # dummy standard number to be replaced when most recent standard version is found\n",
    "        for standardRowNumber in range(1, len(standards_versions_metadata)):\n",
    "            # the row is one of the versions of the standard\n",
    "            if standards_versions_metadata[standardRowNumber][standard_uri] == standardUri:\n",
    "                # Make the version of the row the mostRecent if it's later than the previous mostRecent\n",
    "                if standards_versions_metadata[standardRowNumber][version_uri] > mostRecent:\n",
    "                    mostRecent = standards_versions_metadata[standardRowNumber][version_uri]\n",
    "                    mostRecentStandardNumber = standardRowNumber\n",
    "\n",
    "        # change the status of the most recent standard to superseded\n",
    "        standards_versions_metadata[mostRecentStandardNumber][status_column] = 'superseded'\n",
    "        standards_versions_metadata[mostRecentStandardNumber][document_modified] = isoTime(local_offset_from_utc)\n",
    "\n",
    "        # start the new standard row with the metadata from the most recent vocabulary\n",
    "        newStandardRow = copy.deepcopy(standards_versions_metadata[mostRecentStandardNumber])\n",
    "\n",
    "    # substitute metadata to make the most recent standard version have the modified dates for the new standard version\n",
    "    newStandardRow[document_modified] = isoTime(local_offset_from_utc)\n",
    "    newStandardRow[version_uri] = standardVersionUri\n",
    "    newStandardRow[version_issued] = date_issued\n",
    "    newStandardRow[status_column] = 'recommended'\n",
    "\n",
    "    # append the new term list row to the old list of term lists\n",
    "    standards_versions_metadata.append(newStandardRow)\n",
    "\n",
    "    # save as a file\n",
    "    writeCsv('../standards-versions/standards-versions.csv', standards_versions_metadata)\n",
    "\n",
    "# If this is the second term list change for a new standard version, the previous vocabulary version will have \n",
    "# been added.  So in that case the vocabulary versions need to be checked to prevent duplication.\n",
    "\n",
    "if not alreadyAddedStandard:\n",
    "    # create a list of every vocabulary version that was in the most recent previous standard version\n",
    "    newStandardMembersList = []\n",
    "    # create a corresponding list of local names for those term list versions\n",
    "    vocabularyLocalNameList = []\n",
    "\n",
    "    if aNewStandard:\n",
    "        # the new vocabulary version needs to be added to the list\n",
    "        newStandardMembersList.append(vocabularyVersionUri)\n",
    "    else:\n",
    "        # find the vocabulary versions for the most recent standard version\n",
    "        for vocabularyVersion in standards_versions_parts:\n",
    "            # the first column contains the standard version\n",
    "            if standards_versions_metadata[mostRecentStandardNumber][version_uri] == vocabularyVersion[0]:\n",
    "                newStandardMembersList.append(vocabularyVersion[1])\n",
    "\n",
    "                # dissect the vocabulary version URI to pull out the local name of the vocabulary version\n",
    "                pieces = vocabularyVersion[1].split('/')\n",
    "                versionLocalNamePiece = pieces[len(pieces)-2]\n",
    "                vocabularyLocalNameList.append(versionLocalNamePiece)\n",
    "\n",
    "        if aNewVocabulary:\n",
    "            # the new vocabulary version needs to be added to the list\n",
    "            newStandardMembersList.append(vocabularyVersionUri)\n",
    "        else:\n",
    "            # For the modified vocabulary, find its previous version and replace it with the new version.\n",
    "            for vocabularyVersionRowNumber in range(0, len(newStandardMembersList)):\n",
    "                if vocab_subpath == vocabularyLocalNameList[vocabularyVersionRowNumber]:\n",
    "                    # change the vocabulary version on the list to the new one\n",
    "                    newStandardMembersList[vocabularyVersionRowNumber] = vocabularyVersionUri\n",
    "\n",
    "    # Now that the list of new vocabulary versions that are part of the new standard version list is created,\n",
    "    # add a record for each one to the standard versions members table\n",
    "    for vocabularyVersionMember in newStandardMembersList:\n",
    "        standards_versions_parts.append([standardVersionUri, vocabularyVersionMember])\n",
    "        \n",
    "# In the case where previous vocabulary versions have already been added and a new standard version already generated\n",
    "# we only need to update the new vocabulary version\n",
    "else:\n",
    "    if aNewVocabulary:\n",
    "        # the new vocabulary version needs to be added to the list\n",
    "        standards_versions_parts.append([standardVersionUri, vocabularyVersionUri])\n",
    "    else:\n",
    "        # in this case a vocabulary is modified rather than new. So find its version under the current standard version\n",
    "        # and replace it with the new vocabulary version.  If the change was to a different term list but in the same\n",
    "        # standard, that's fine - the vocabulary version will be replaced with the same one and duplication will still\n",
    "        # be prevented\n",
    "        for vocabularyVersionRowNumber in range(0, len(standards_versions_parts)):\n",
    "            # consider only vocabularies that match the standard version URI\n",
    "            if standards_versions_parts[vocabularyVersionRowNumber][0] == standardVersionUri:\n",
    "                # dissect the vocabulary version URI to pull out the local name of the vocabulary version\n",
    "                pieces = standards_versions_parts[vocabularyVersionRowNumber][1].split('/')\n",
    "                versionLocalNamePiece = pieces[len(pieces)-2]\n",
    "                # check for a match of the vocabulary version local name with the vocabulary string\n",
    "                if versionLocalNamePiece == vocabulary:\n",
    "                    # change the vocabulary version on the list to the new one\n",
    "                    standards_versions_parts[vocabularyVersionRowNumber][1] = vocabularyVersionUri\n",
    "\n",
    "# Write the updated vocabularies versions members table to a file\n",
    "writeCsv('../standards-versions/standards-versions-parts.csv', standards_versions_parts)\n",
    "\n",
    "if not(aNewStandard) and not(alreadyAddedStandard):\n",
    "    standards_versions_replacements.append([standardVersionUri, standards_versions_metadata[mostRecentStandardNumber][version_uri]])\n",
    "    writeCsv('../standards-versions/standards-versions-replacements.csv', standards_versions_replacements)\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
