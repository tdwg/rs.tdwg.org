{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update TDWG documents metadata\n",
    "\n",
    "Author: Steve Baskauf - 2022-05-27\n",
    "\n",
    "Version: 0.1\n",
    "\n",
    "This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "\n",
    "This script is a companion to the other script that updates the vocabularies metadata and should be run after it is finished and any new list of terms documents have been created.\n",
    "\n",
    "After some use and testing, this will get turned from a Jupyter notebook to a stand-alone script. \n",
    "\n",
    "NOTE: Creating new documents metadata depends on several YAML configuration files that are located in a subdirectory, but which need to be moved to the same directory as this script to be used. To generate them based on existing documents or to use an existing document as a template for a new document, there are some cells at the end of the script that can be used. However, running them after editing the configuration files manually will overwrite your manual changes.\n",
    "\n",
    "When the script is run to update existing documents, the YAML files aren't needed unless changes are being made. In the absence of those configuration files, the previous data will be used.\n",
    "\n",
    "## Configuration and function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import json\n",
    "import sys\n",
    "import copy\n",
    "from os.path import exists\n",
    "\n",
    "def csv_read(path, **kwargs):\n",
    "    \"\"\"Loads a CSV table into a Pandas DataFrame with all cells as strings and blank cells as empty strings\n",
    "    \n",
    "    Keyword argument:\n",
    "    rows -- the number of rows of the table to return when used for testing. When omitted, all rows are returned.\n",
    "    \"\"\"\n",
    "    dataframe = pd.read_csv(path, na_filter=False, dtype = str)\n",
    "    if 'rows' in kwargs:\n",
    "        return dataframe.head(kwargs['rows']).copy(deep=True)\n",
    "    else:\n",
    "        return dataframe\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load document data\n",
    "\n",
    "If the document already exists, its data is retrieved from current documents CSV. A `document_configuration.yaml` file provides new data, which replaces any existing data or is used to create a new record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_date = '2022-05-26'\n",
    "doc_config_path = 'document_configuration.yaml'\n",
    "format_config_path = 'format_configuration.yaml'\n",
    "repo_path = '../../rs.tdwg.org/'\n",
    "author_config_path = 'authors_configuration.yaml'\n",
    "#doc_iri = 'http://rs.tdwg.org/sds/doc/specification/'\n",
    "doc_iri = 'http://rs.tdwg.org/ac/doc/blah/'\n",
    "standard_iri = 'http://www.tdwg.org/standards/638'\n",
    "new_accessUrl = ''\n",
    "\n",
    "current_docs_df = csv_read(repo_path + 'docs/docs.csv')\n",
    "\n",
    "# Find the row index if the document already exists\n",
    "row_matches = current_docs_df.index[current_docs_df['current_iri']==doc_iri].tolist()\n",
    "if len(row_matches) == 0:\n",
    "    print('Document IRI not found in existing data.')\n",
    "    new_document = True\n",
    "elif len(row_matches) > 1:\n",
    "    sys.exit('Multiple rows match the document IRI:' + str(row_matches))\n",
    "else:\n",
    "    row_index = row_matches[0]\n",
    "    new_document = False\n",
    "\n",
    "    # .squeeze() turns a single-row or column dataframe into a series.\n",
    "    # See https://stackoverflow.com/questions/50575802/convert-dataframe-row-to-dict\n",
    "    # and https://www.w3resource.com/pandas/dataframe/dataframe-squeeze.php\n",
    "    row_data = current_docs_df[current_docs_df['current_iri']==doc_iri].squeeze().to_dict()\n",
    "\n",
    "# Try to load new document data from a configuration file.\n",
    "if exists(doc_config_path):\n",
    "    with open(doc_config_path) as file_object:\n",
    "        new_row_data = yaml.safe_load(file_object)\n",
    "    \n",
    "    # Need to stash any new accessUrl that is provided\n",
    "    if new_row_data['accessUrl'] != None: # Empty YAML values are read in as a None keyword.\n",
    "        new_accessUrl = new_row_data['accessUrl']\n",
    "\n",
    "    # For new documents, the data from the file is used as the initial record.\n",
    "    if new_document:\n",
    "        row_data = new_row_data\n",
    "    # For existing documents, any new data replaces the existing data.\n",
    "    else:\n",
    "        for key in new_row_data.keys():\n",
    "            if new_row_data[key] != None: # Empty YAML values are read in as a None keyword.\n",
    "                row_data[key] = new_row_data[key]\n",
    "    \n",
    "else:\n",
    "    # If the document is new but there isn't a config file, there are no data to work with for the document\n",
    "    if new_document:\n",
    "        sys.exit('New documents must have a document_configuration.yaml file.')\n",
    "\n",
    "# Try to load format data from a configuration file.\n",
    "if exists(format_config_path):\n",
    "    with open(format_config_path) as file_object:\n",
    "        format_data = yaml.safe_load(file_object)\n",
    "else:\n",
    "    format_data['mediaType'] = ''\n",
    "    format_data['lastVersionAccessUri'] = ''\n",
    "    \n",
    "#print(json.dumps(format_data, indent=2))\n",
    "\n",
    "# Replace any existing doc_modified date with the new version date\n",
    "row_data['doc_modified'] = version_date\n",
    "#print(json.dumps(row_data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write new data to the current documents CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_document: # If it's a new document, the row data gets added to the end of the DataFrame\n",
    "    # Constructs a one-row DataFrame from a list containing a single dict, then concatenates it to the end\n",
    "    # of the existing DataFrame.\n",
    "    current_docs_df = pd.concat([current_docs_df, pd.DataFrame([row_data])])\n",
    "else: # The new values of the row cells replace the old one.\n",
    "    for key in row_data:\n",
    "        current_docs_df.at[row_index, key] = row_data[key]\n",
    "\n",
    "current_docs_df.to_csv(repo_path + 'docs/docs.csv', index = False)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the documents versions metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new version for the document based on the current document IRI and version_date.\n",
    "doc_version_iri = row_data['current_iri'] + version_date\n",
    "\n",
    "# Load versions list and find most recent version if not a new document.\n",
    "versions_list_df = csv_read(repo_path + 'docs/docs-versions.csv')\n",
    "if not new_document:\n",
    "    matching_versions = versions_list_df[versions_list_df['current_iri']==doc_iri]\n",
    "    matching_versions = matching_versions.sort_values(by=['version_iri'], ascending=[False])\n",
    "    most_recent_version_iri = matching_versions.iat[0, 1]\n",
    "#print(most_recent_version_iri)\n",
    "\n",
    "# Update the list of document versions in the docs folder\n",
    "version_row_data = {'current_iri': row_data['current_iri'], 'version_iri': doc_version_iri}\n",
    "versions_list_df = pd.concat([versions_list_df, pd.DataFrame([version_row_data])])\n",
    "\n",
    "#versions_list_df.to_csv(repo_path + 'docs/docs-versions.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangle current document metadata row dictionary to match the versions metadata column headers\n",
    "versions_data = copy.deepcopy(row_data)\n",
    "\n",
    "del versions_data['doc_created']\n",
    "del versions_data['doc_modified']\n",
    "versions_data['version_issued'] = version_date\n",
    "versions_data['version_iri'] = doc_version_iri\n",
    "versions_data['mediaType'] = format_data['mediaType']\n",
    "\n",
    "# Update the document versions metadata in the docs-versions folder\n",
    "versions_metadata_df = csv_read(repo_path + 'docs-versions/docs-versions.csv')\n",
    "versions_metadata_df = pd.concat([versions_metadata_df, pd.DataFrame([versions_data])])\n",
    "versions_metadata_df.to_csv(repo_path + 'docs-versions/docs-versions.csv', index = False)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the versions replacements unless the document is new\n",
    "if not new_document:\n",
    "    versions_replacements_df = csv_read(repo_path + 'docs-versions/docs-versions-replacements.csv')\n",
    "    replacement_row_data = {'replacing_document': doc_version_iri, 'replaced_document': most_recent_version_iri}\n",
    "    versions_replacements_df = pd.concat([versions_replacements_df, pd.DataFrame([replacement_row_data])])\n",
    "    versions_replacements_df.to_csv(repo_path + 'docs-versions/docs-versions-replacements.csv', index = False)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the access URLs and media types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load format information\n",
    "formats_metadata_df = csv_read(repo_path + 'docs/docs-formats.csv')\n",
    "\n",
    "# Look for the previously used format information for this doc\n",
    "if not new_document:\n",
    "    old_accessUrl = formats_metadata_df.loc[formats_metadata_df.doc_iri == doc_iri, 'accessUri'].values[0]\n",
    "    old_mediaType = formats_metadata_df.loc[formats_metadata_df.doc_iri == doc_iri, 'mediaType'].values[0]\n",
    "    \n",
    "#print(old_accessUrl)\n",
    "#print(old_mediaType)\n",
    "\n",
    "# If there is a newly provided access URL and media type for the current document, use it.\n",
    "# Otherwise use the old one.\n",
    "\n",
    "# NOTE: if it's a new document, a new accessUrl must be provided along with the rest of the metadata.\n",
    "# If that isn't done, the script here doesn't handle it and will throw an error later when current_accessUrl\n",
    "# doesn't have a value.\n",
    "if new_accessUrl:\n",
    "    current_accessUrl = new_accessUrl\n",
    "else:\n",
    "    current_accessUrl = old_accessUrl\n",
    "\n",
    "if format_data['mediaType']:\n",
    "    current_mediaType = format_data['mediaType']\n",
    "else:\n",
    "    try:\n",
    "        current_mediaType = old_mediaType\n",
    "    # Handle the case where the creator of a new document doesn't bother to create the format config file\n",
    "    except: # We assume the document is in Markdown if no information is given\n",
    "        current_mediaType = 'text/markdown'\n",
    "        \n",
    "# For pre-existing documents, we try to replace the values of the accessUrl and mediaType, which might change.\n",
    "if not new_document:\n",
    "    # Find the row for the pre-existing document\n",
    "    not_found = False\n",
    "    row_matches = formats_metadata_df.index[formats_metadata_df['doc_iri']==doc_iri].tolist()\n",
    "    if len(row_matches) == 0:\n",
    "        not_found = True # If not previously present, we'll add it as if it were a new document and fix it.\n",
    "    else:\n",
    "        if len(row_matches) > 1:\n",
    "            print('Warning: Multiple rows in the docs-formats.csv file match the document IRI:' + str(row_matches))\n",
    "            row_index = row_matches[0]\n",
    "        else:\n",
    "            row_index = row_matches[0]\n",
    "        # Now make the replacements\n",
    "        formats_metadata_df.at[row_index, 'mediaType'] = current_mediaType\n",
    "        formats_metadata_df.at[row_index, 'accessUri'] = current_accessUrl\n",
    "        \n",
    "# Cases where we need to add a row because the media type wasn't there before  \n",
    "if new_document or not_found:\n",
    "    format_row_data = {'doc_iri': doc_iri, 'mediaType': current_mediaType, 'accessUri': current_accessUrl}\n",
    "    formats_metadata_df = pd.concat([formats_metadata_df, pd.DataFrame([format_row_data])])\n",
    "\n",
    "# Now save the updated table\n",
    "formats_metadata_df.to_csv(repo_path + 'docs/docs-formats.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load format information for versions.\n",
    "versions_format_metadata_df = csv_read(repo_path + 'docs-versions/docs-versions-formats.csv')\n",
    "\n",
    "# The previous version usually needs to have it's access URL changed since it's not the current version webpage any more.\n",
    "if not new_document:\n",
    "    # Find the row for the pre-existing document\n",
    "    not_found = False\n",
    "    row_matches = versions_format_metadata_df.index[versions_format_metadata_df['version_iri']==most_recent_version_iri].tolist()\n",
    "    if len(row_matches) == 0:\n",
    "        print('no match found')\n",
    "        not_found = True # If not previously present, we'll add it as if it were a new document and fix it.\n",
    "    else:\n",
    "        if len(row_matches) > 1:\n",
    "            print('Warning: Multiple rows in the docs-versions-formats.csv file match the document IRI:' + str(row_matches))\n",
    "            row_index = row_matches[0]\n",
    "        else:\n",
    "            row_index = row_matches[0]\n",
    "        # Now make the replacement, using the URL provided in the format config file, if it was provided.\n",
    "        # If it wasn't provided, then whatever URL was already there will remain.\n",
    "        if format_data['lastVersionAccessUri']:\n",
    "            versions_format_metadata_df.at[row_index, 'accessUri'] = format_data['lastVersionAccessUri']\n",
    "            \n",
    "# Handle the edge case where the row for the previous document is missing.\n",
    "# Doesn't error trap the case where the old access URI isn't provided, but hey, it's an edge case and be more careful.\n",
    "if not_found:\n",
    "    versions_format_row_data = {'version_iri': most_recent_version_iri, 'mediaType': old_mediaType, 'accessUri': format_data['lastVersionAccessUri']}\n",
    "    versions_format_metadata_df = pd.concat([versions_format_metadata_df, pd.DataFrame([versions_format_row_data])])\n",
    "\n",
    "# For versions, a new row is always added to the file\n",
    "versions_format_row_data = {'version_iri': doc_version_iri, 'mediaType': current_mediaType, 'accessUri': current_accessUrl}\n",
    "versions_format_metadata_df = pd.concat([versions_format_metadata_df, pd.DataFrame([versions_format_row_data])])\n",
    "\n",
    "versions_format_metadata_df.to_csv(repo_path + 'docs-versions/docs-versions-formats.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update data about authors\n",
    "\n",
    "Behaviors:\n",
    "1. If there is a configuration file, it gets used as-is. \n",
    "- For new documents, the authors get added. This is also true for docs-roles.csv .\n",
    "- For existing documents, the data from the config file replaces the existing data for the current doc. Also true for docs-roles.csv .\n",
    "2. If there is no configuration file, the current doc data is unchanged. The previous author information gets used for the new version. No change is made to the docs-roles.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing author data\n",
    "authors_df = csv_read(repo_path + 'docs/docs-authors.csv')\n",
    "roles_df = csv_read(repo_path + 'docs-roles/docs-roles.csv')\n",
    "\n",
    "# Try to load new document data from a configuration file.\n",
    "# For new documents, the data from the YAML file must be used as the initial record.\n",
    "if exists(author_config_path):\n",
    "    # Load the new author data from the YAML file\n",
    "    with open(author_config_path) as file_object:\n",
    "        author_data = yaml.safe_load(file_object)\n",
    "    for author_number in range(len(author_data)):\n",
    "        # Need to add in the document column\n",
    "        author_data[author_number]['document'] = doc_iri\n",
    "        \n",
    "        # Need to turn None values into empty strings\n",
    "        for key in author_data[author_number].keys():\n",
    "            if author_data[author_number][key] == None: # Empty YAML values are read in as a None keyword.\n",
    "                author_data[author_number][key] = ''\n",
    "        \n",
    "    #print(json.dumps(new_author_data, indent=2))\n",
    "    \n",
    "    if not new_document:\n",
    "        # For existing documents, any new data replaces the existing data.\n",
    "        # Remove existing rows where the doc IRI matches, then add in new author data\n",
    "        authors_df = authors_df[authors_df['document']!=doc_iri]\n",
    "        roles_df = roles_df[roles_df['document']!=doc_iri]\n",
    "        \n",
    "    # Write the modified author DataFrame back out to the authors data file\n",
    "    authors_df = pd.concat([authors_df, pd.DataFrame(author_data)])    \n",
    "    authors_df.to_csv(repo_path + 'docs/docs-authors.csv', index = False)\n",
    "    \n",
    "    # The new (or replacement) rows for docs-roles.csv need to be constructed.\n",
    "    roles_list = []\n",
    "    for author in author_data:\n",
    "        roles_dict = {'document': doc_iri, 'contributor_role': author['contributor_role'], 'contributor_literal': author['contributor_literal']}\n",
    "        # Put the author IRI in the column that corresponds to their role\n",
    "        contributor_role_column_header = author['contributor_role'].replace(' ', '_') # column headers don't have spaces\n",
    "        roles_dict[contributor_role_column_header] = author['contributor_iri']\n",
    "        # Perform a check to warn if the author's role isn't one that's already represented in the columns of the CSV\n",
    "        if not contributor_role_column_header in roles_df.columns:\n",
    "            print('WARNING: author', author['contributor_literal'], 'has the role', author['contributor_role'], 'that is not an existing column in the docs-roles.csv file')\n",
    "        roles_list.append(roles_dict)\n",
    "    # Now add the generated rows to the end of the dataframe and save\n",
    "    roles_df = pd.concat([roles_df, pd.DataFrame(roles_list)])    \n",
    "    roles_df.to_csv(repo_path + 'docs-roles/docs-roles.csv', index = False)    \n",
    "        \n",
    "else: # No new author data found, use existing data. The authors of the current documents (docs-authors.csv) are unchanged.\n",
    "    # Load the existing data from the CSV\n",
    "    author_data = []\n",
    "    for index, row in authors_df.iterrows():\n",
    "        # The row is a Pandas series whose items can be referenced by their identifiers (from the column headers)\n",
    "        if row['document']==doc_iri:\n",
    "            row_dict = row.to_dict()\n",
    "            author_data.append(row_dict)\n",
    "            \n",
    "    #print(json.dumps(rows_list, indent=2))\n",
    "    \n",
    "# Create author records for the new version\n",
    "versions_author_metadata_df = csv_read(repo_path + 'docs-versions/docs-versions-authors.csv')\n",
    "\n",
    "# In each row of the new metadata, change the \"document\" column to the \"document-version\" column with a new IRI\n",
    "versions_author_data = []\n",
    "for author_dict in author_data:\n",
    "    del author_dict['document']\n",
    "    author_dict['document_version'] = doc_version_iri\n",
    "    versions_author_data.append(author_dict)\n",
    "\n",
    "# Now add the modified versions author data to the original DataFrame\n",
    "versions_author_metadata_df = pd.concat([versions_author_metadata_df, pd.DataFrame(versions_author_data)])\n",
    "versions_author_metadata_df.to_csv(repo_path + 'docs-versions/docs-versions-authors.csv', index = False)\n",
    "    \n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update standards components with doc information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_document:\n",
    "    # Load existing standards data\n",
    "    stds_parts_df = csv_read(repo_path + 'standards/standards-parts.csv')\n",
    "    \n",
    "    # Add a new row for the new document\n",
    "    stds_parts_row_data = {'standard': standard_iri, 'part': doc_iri, 'rdf_type': 'foaf:Document'}\n",
    "    stds_parts_df = pd.concat([stds_parts_df, pd.DataFrame([stds_parts_row_data])])\n",
    "\n",
    "    stds_parts_df.to_csv(repo_path + 'standards/standards-parts.csv', index = False)\n",
    "\n",
    "# Load existing standards versions data\n",
    "stds_version_parts_df = csv_read(repo_path + 'standards-versions/standards-versions-parts.csv')\n",
    "\n",
    "# Add a new row for the new document version\n",
    "stds_version_parts_row_data = {'standard_version': standard_iri + '/version/' + version_date, 'part': doc_version_iri}\n",
    "stds_version_parts_df = pd.concat([stds_version_parts_df, pd.DataFrame([stds_version_parts_row_data])])\n",
    "\n",
    "stds_version_parts_df.to_csv(repo_path + 'standards-versions/standards-versions-parts.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't run the next two cells\n",
    "\n",
    "The cells can be used to generate template YAML configuration files from existing rows in the table, but most times you won't need to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for documents\n",
    "\n",
    "# The doc_iri determines the row of the table to be used to generate the sample\n",
    "doc_iri = 'http://rs.tdwg.org/ac/doc/termlist/'\n",
    "\n",
    "current_docs_df = csv_read(repo_path + 'docs/docs.csv')\n",
    "\n",
    "# Find the row index if the document already exists\n",
    "row_matches = current_docs_df.index[current_docs_df['current_iri']==doc_iri].tolist()\n",
    "if len(row_matches) == 0:\n",
    "    print('Document IRI not found in existing data.')\n",
    "    new_document = True\n",
    "elif len(row_matches) > 1:\n",
    "    sys.exit('Multiple rows match the document IRI:' + str(row_matches))\n",
    "else:\n",
    "    row_index = row_matches[0]\n",
    "    new_document = False\n",
    "\n",
    "    # .squeeze() turns a single-row or column dataframe into a series.\n",
    "    # See https://stackoverflow.com/questions/50575802/convert-dataframe-row-to-dict\n",
    "    # and https://www.w3resource.com/pandas/dataframe/dataframe-squeeze.php\n",
    "    row_data = current_docs_df[current_docs_df['current_iri']==doc_iri].squeeze().to_dict()\n",
    "\n",
    "with open('document_configuration.yaml', 'w', encoding = \"utf-8\") as file_object:\n",
    "    dump = yaml.dump(row_data, allow_unicode=True, sort_keys=False)\n",
    "    file_object.write(dump)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for authors\n",
    "\n",
    "# The doc_iri determines the rows of the table to be used to generate the sample\n",
    "doc_iri = 'http://rs.tdwg.org/ac/doc/termlist/'\n",
    "\n",
    "current_docs_df = csv_read(repo_path + 'docs/docs-authors.csv')\n",
    "\n",
    "rows_list = []\n",
    "for index, row in current_docs_df.iterrows():\n",
    "    # The row is a Pandas series whose items can be referenced by their identifiers (from the column headers)\n",
    "    if row['document']==doc_iri:\n",
    "        row_dict = row.to_dict()\n",
    "        del row_dict['document']\n",
    "        rows_list.append(row_dict)\n",
    "\n",
    "with open('authors_configuration.yaml', 'w', encoding = \"utf-8\") as file_object:\n",
    "    #dump = yaml.dump(rows_list)\n",
    "    dump = yaml.dump(rows_list, allow_unicode=True, sort_keys=False)\n",
    "    dump = dump.replace('\\n-', '\\n\\n-') # Insert extra newline between records\n",
    "    file_object.write(dump)\n",
    "\n",
    "#print(json.dumps(rows_list, indent =2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
